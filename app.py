import streamlit as st
import csv
import os
import numpy as np
import pandas as pd
from tqdm import tqdm
from PIL import Image
import torch
import clip
from natsort import natsorted
from typing import List, Tuple
import json

# L∆∞u tr·∫°ng th√°i v√†o t·ªáp tin JSON
def save_session_state(session_state, filename):
    with open(filename, 'w') as file:
        json.dump(session_state, file)

# ƒê·ªçc tr·∫°ng th√°i t·ª´ t·ªáp tin JSON
def load_session_state(filename):
    with open(filename, 'r') as file:
        session_state = json.load(file)
        return session_state

##Set up page
st.set_page_config(
    page_title="Kumo Search",
    page_icon=":üï∑:",
    layout="wide",
)

#Set sidebar:
with st.sidebar:
    st.header(":blue[KUMO]:snow_cloud:", divider='rainbow')

    st.write("Member:\n")
    image = Image.open('DyThen.jpg')
    image = image.resize((200, 200))
    st.image(image, caption='Nguy·ªÖn Duy Th·∫Øng')

    st.write("üï∏Ô∏è Contact us:\n")
    st.write("‚úâ Email: 22521333@gm.uit.edu.vn\n")
    st.write("Û†ÅÜÛ†ÅÜ‚ìï Facebook: https://www.facebook.com/dythen.kumo")

#Set logo
col = st.columns(4)
with col[0]:
    img = Image.open('AI-Challange.jpg')
    img = img.resize((100, 100))
    st.image(img)
with col[1]:
    img = Image.open('UIT.jpg')
    img = img.resize((100, 100))
    st.image(img)
with col[2]:
    img = Image.open('AI-Club.jpg')
    img = img.resize((100, 100))
    st.image(img)
with col[3]:
    img = Image.open('Kumo.jpg')
    img = img.resize((100, 100))
    st.image(img)

#T·∫°o model x·ª≠ l√Ω text
class TextEmbedding():
  def __init__(self):
    self.device = "cuda" if torch.cuda.is_available() else "cpu"
    self.model, _ = clip.load("ViT-B/32", device=self.device)

  def __call__(self, text: str) -> np.ndarray:
    text_inputs = clip.tokenize([text]).to(self.device)
    with torch.no_grad():
        text_feature = self.model.encode_text(text_inputs)[0]

    return text_feature.detach().cpu().numpy()

#T·∫°o model x·ª≠ l√Ω ·∫£nh
class ImageEmbedding():
    def __init__(self):
        self.device = "cpu"
        self.model, self.preprocess = clip.load("ViT-B/32", device=self.device)

    def __call__(self, image_path: str) -> np.ndarray:
        image = Image.open(image_path).convert("RGB")
        image_input = self.preprocess(image).unsqueeze(0).to(self.device)

        with torch.no_grad():
            image_feature = self.model.encode_image(image_input)[0]

        return image_feature.detach().cpu().numpy()

#Load clip-feature
@st.cache_data
def indexing_methods(features_path: str) -> pd.DataFrame:
    data = {'video_name': [], 'frame_index': [], 'features_vector': [], 'features_dimension': []}

    npy_files = natsorted([file for file in os.listdir(features_path) if file.endswith(".npy")])

    for feat_npy in tqdm(npy_files):
        video_name = feat_npy.split('.')[0]
        feats_arr = np.load(os.path.join(features_path, feat_npy))

        # L·∫∑p qua t·ª´ng d√≤ng trong feats_arr, m·ªói d√≤ng l√† m·ªôt frame
        for idx, feat in enumerate(feats_arr):
            data['video_name'].append(video_name)
            data['frame_index'].append(idx)
            data['features_vector'].append(feat)
            data['features_dimension'].append(feat.shape)

    df = pd.DataFrame(data)
    return df
#Load data
FEATURES_PATH = r"D:\CLB-AI\AI Challenge 2023\Data\clip-features-vit-b32"
visual_features_df = indexing_methods(FEATURES_PATH)

#T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa c√°c vector
def similar(query_arr, feat_vec, measure_method):
    distance = 0
    if measure_method == "cosine_similarity":
        dot_product = query_arr @ feat_vec
        query_norm = np.linalg.norm(query_arr)
        feat_norm = np.linalg.norm(feat_vec)
        cosine_similarity = dot_product / (query_norm * feat_norm)
        distance = 1 - cosine_similarity
    elif measure_method == "euclid":
        distance = np.linalg.norm(query_arr - feat_vec)
    else:
        distance = np.linalg.norm(query_arr - feat_vec, ord=1)

    return distance

#H√†m t√¨m ki·∫øm
def search_engine(query_arr: np.array,
                  db: list,
                  topk:int=10,
                  measure_method: str="cosine_similarity") -> List[dict]:
    measure = []
    for ins_id, instance in enumerate(db):
        video_name, idx, feat_vec, feat_dim = instance

        distance = similar(query_arr, feat_vec, measure_method)

        measure.append((ins_id, distance))

    measure = sorted(measure, key=lambda x:x[1])

    search_result = []
    for instance in measure[:topk]:
        ins_id, distance = instance
        video_name, idx = db[ins_id][0], db[ins_id][1]

        search_result.append({"video_name": video_name,
                              "keyframe_id": idx,
                              "score": distance})

    # ƒê·∫£m b·∫£o tr·∫£ v·ªÅ ƒë√∫ng topk k·∫øt qu·∫£
    while len(search_result) < topk and len(measure) > len(search_result):
        ins_id, distance = measure[len(search_result)]
        video_name, idx = db[ins_id][0], db[ins_id][1]
        search_result.append({"video_name": video_name,
                              "keyframe_id": idx,
                              "score": distance})

    return search_result

#H√†m ƒë·ªçc ·∫£nh t√¨m ƒë∆∞·ª£c
def read_image(results: List[dict]):
    images = []
    data = []
    IMAGE_KEYFRAME_PATH = r"D:\CLB-AI\AI Challenge 2023\Data\Keyframes"  # ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a keyframes

    for res in results:
        data.append(res)
        video_name = res["video_name"]
        keyframe_id = res["keyframe_id"]
        temp = video_name.split('_')
        video_folder = os.path.join(IMAGE_KEYFRAME_PATH, 'Keyframes_' + temp[0], 'keyframes', video_name)

        if os.path.exists(video_folder):
            image_files = sorted(os.listdir(video_folder))

            if keyframe_id < len(image_files):
                image_file = image_files[keyframe_id]
                image_path = os.path.join(video_folder, image_file)
                image = Image.open(image_path)
                images.append(image)
            else:
                st.write(f"Keyframe id {keyframe_id} is out of range for video {video_name}.")

    return (images, data)

#H√†m t·∫°o file n·ªôp
def convert_df(lst):
    df = pd.DataFrame(lst)
    return df.to_csv(index=False, header=False).encode('utf-8')

MAP_KEYFRAME_PATH = r"D:\CLB-AI\AI Challenge 2023\Data\map-keyframes"
def submit(answer, name):
    if len(answer) > 0:
        last_answer = []
        #T√¨m l·∫°i ƒë√∫ng keyframe_idx
        for ans in answer:
            namefile = str(ans[0]) + '.csv'
            path_name = os.path.join(MAP_KEYFRAME_PATH, namefile)
            df = pd.read_csv(path_name)
            
            temp = [ans[0], df['frame_idx'][ans[1]]]
            last_answer.append(temp)
            # #Th√™m c√°c keyframe l√¢n c·∫≠n
            # temp = [ans[0], df['frame_idx'][ans[1] - 1]]
            # last_answer.append(temp)
            # temp = [ans[0], df['frame_idx'][ans[1] + 1]]
            # last_answer.append(temp)

        #T·∫°o file ƒë·ªÉ n·ªôp
        filename = 'query-p3-' + str(name) + '.csv'
        file = convert_df(last_answer)
        st.download_button(label="Download data as CSV",
                        data=file,
                        file_name=filename,
                        mime='text/csv',)

#H√†m hi·ªÉn th·ªã ·∫£nh 
def visualize(imgs,data):
    columns = st.columns(5)
    for i, img in enumerate(imgs):
        with columns[i%5]:
            name = "Keyframe-result number {}: ".format(i+1) + str(data[i]["video_name"]) + ' ' + str(data[i]["keyframe_id"])
            temp = img.resize((320, 180))
            st.image(temp, caption=name)    
#K·∫øt th√∫c ph·∫ßn kh·ªüi t·∫°o cho back-end

##B·∫Øt ƒë·∫ßu truy v·∫•n
text_embedd = TextEmbedding()
file_name = 'Session.json'
# Thay ƒë·ªïi c√°c gi√° tr·ªã d·ª±a tr√™n t√πy ch·ªçn c·ªßa ng∆∞·ªùi d√πng
with st.form('Nh·∫≠p c√°c parameter: '):
    text_query = st.text_input('M√¥ t·∫£ (vui l√≤ng vi·∫øt b·∫±ng ti·∫øng Anh): ')
    topk = st.slider('S·ªë l∆∞·ª£ng k·∫øt qu·∫£ t√¨m ki·∫øm', 0, 50, 15)
    measure_method = st.selectbox('Ch·ªçn ph∆∞∆°ng th·ª©c ƒëo ch√≠nh: ', ('cosine_similarity', 'mahatan', 'euclid'))
    submit_button = st.form_submit_button("OK")

st.info('Kumo c√≥ th·ªÉ m·∫•t v√†i ph√∫t ƒë·ªÉ l√†m vi·ªác', icon="üï∑")
#L∆∞u l·∫°i phi√™n l√†m vi·ªác
if submit_button:
    session = {'run': True}
    save_session_state(session_state= session, filename=file_name)

# B·∫Øt ƒë·∫ßu ch∆∞∆°ng tr√¨nh
session = load_session_state(filename=file_name)
if session['run']:
    with st.spinner('Vui l√≤ng ch·ªù m·ªôt ch√∫t: '):
        # T·∫°o vector bi·ªÉu di·ªÖn cho c√¢u truy v·∫•n vƒÉn b·∫£n
        text_feat_arr = text_embedd(text_query)
        # Chuy·ªÉn DataFrame th√†nh danh s√°ch tuples
        visual_features_db = visual_features_df.to_records(index=False).tolist()
        # Th·ª±c hi·ªán t√¨m ki·∫øm v√† hi·ªÉn th·ªã k·∫øt qu·∫£
        search_result = search_engine(text_feat_arr, visual_features_db, topk, measure_method)
        (images, data) = read_image(search_result)
        visualize(images, data)
        st.success('Ho√†n t·∫•t truy v·∫•n!', icon="‚úÖ")
    
    index = []
    for i in range(len(images)):
        index.append(i+1)
    with st.form('Submit parameter: '):
        idx = st.multiselect('Ch·ªçn keyframe ƒë·ªÉ submit:', index)
        #Ch·ªçn t√™n querry ƒë·ªÉ t·∫°o file submit
        name = st.number_input('Ch·ªçn s·ªë th·ª© t·ª± query:',1)
        sub = st.form_submit_button('Submit')

    answer = []
    if sub:
        for i in idx:
            temp = [data[i-1]["video_name"], data[i-1]["keyframe_id"]]
            answer.append(temp)

    submit(answer, name)
    #K·∫øt th√∫c ch∆∞∆°ng tr√¨nh, ƒë·∫∑t l·∫°i gi√° tr·ªã
    if st.button('K·∫øt th√∫c truy v·∫•n'):
        session = {'run': False}
        save_session_state(session_state= session, filename=file_name)